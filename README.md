# Simple QA Dataset Processor

Инструмент для обогащения датасета вопросов-ответов (QA) путем автоматического скачивания и извлечения текста из документов по URL из метаданных.

## Описание

Проект обрабатывает CSV датасет с вопросами и ответами, извлекает URL документов из поля `metadata`, скачивает эти документы и добавляет их текстовое содержимое в новое поле `documents`. Поддерживаются форматы: HTML, PDF и текстовые файлы.

### Датасеты

- **Исходный датасет**: `simple_qa_test_set.csv` - входной датасет с вопросами, ответами и метаданными (URL документов)
- **Итоговый датасет**: `simple_qa_test_set_with_documents.csv` - обогащенный датасет с добавленным полем `documents`, содержащим тексты скачанных документов

## Возможности

- ✅ Автоматическое скачивание документов по URL
- ✅ Поддержка форматов: HTML, PDF, текстовые файлы
- ✅ Параллельная обработка (до 5 потоков)
- ✅ Кэширование документов для избежания повторных запросов
- ✅ Прогресс-бар с отображением статуса обработки
- ✅ Промежуточное сохранение результатов каждые 100 строк
- ✅ Детальное логирование процесса
- ✅ Генерация итогового отчета
- ✅ Корректная обработка сигналов завершения

## Требования

- Python 3.8+
- Зависимости (устанавливаются через pip):
  - `pandas`
  - `requests`
  - `beautifulsoup4`
  - `PyPDF2`
  - `tqdm`

## Установка

1. Клонируйте репозиторий:
```bash
git clone <repository-url>
cd simple_qa
```

2. Создайте виртуальное окружение:
```bash
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# или
venv\Scripts\activate  # Windows
```

3. Установите зависимости:
```bash
pip install -r requirements.txt
```

Или установите вручную:
```bash
pip install pandas requests beautifulsoup4 PyPDF2 tqdm
```

## Структура входного датасета

Входной CSV файл должен содержать следующие колонки:

- `metadata` - JSON строка с метаданными, включая поле `urls` со списком URL документов
- `problem` - вопрос
- `answer` - ответ

Пример строки:
```csv
metadata,problem,answer
"{'topic': 'Science', 'urls': ['https://example.com/doc1.pdf', 'https://example.com/doc2.html']}",What is the topic?,Science
```

## Использование

### Базовое использование

```bash
python download_documents_full.py
```

По умолчанию скрипт обрабатывает:
- Входной файл: `simple_qa_test_set.csv`
- Выходной файл: `simple_qa_test_set_with_documents.csv`

### Запуск в фоновом режиме

Для длительной обработки больших датасетов используйте скрипт запуска в фоне:

```bash
./run_background.sh
```

Это запустит процесс в фоне и сохранит его PID в `download_pid.txt`.

### Проверка прогресса

Для проверки статуса обработки используйте:

```bash
./check_progress.sh
```

Скрипт покажет:
- Статус процесса (запущен/остановлен)
- Последние записи из логов
- Размер промежуточных и финальных файлов

### Настройка параметров

Вы можете изменить параметры в скрипте `download_documents_full.py`:

```python
# Параметры скачивания
timeout=15          # Таймаут запроса в секундах
delay=0.1          # Задержка между запросами в секундах
max_workers=5      # Количество параллельных потоков

# Пути к файлам
input_file = "/path/to/input.csv"
output_file = "/path/to/output.csv"
```

## Результаты

После завершения обработки создаются следующие файлы:

1. **`simple_qa_test_set_with_documents.csv`** - основной результат с добавленным полем `documents`
2. **`simple_qa_test_set_with_documents_report.txt`** - итоговый отчет со статистикой
3. **`download_progress.log`** - лог процесса обработки

### Структура выходного файла

Выходной CSV содержит все исходные колонки плюс:
- `documents` - список текстов скачанных документов (JSON массив строк)

## Статистика обработки

Отчет включает:
- Общее количество обработанных строк
- Количество успешно обработанных строк
- Количество неудачных загрузок
- Статистику HTTP запросов
- Эффективность кэширования

## Особенности реализации

- **Параллельная обработка**: Используется `ThreadPoolExecutor` для параллельного скачивания документов
- **Кэширование**: Документы кэшируются в памяти для избежания повторных запросов к одинаковым URL
- **Устойчивость**: Промежуточные результаты сохраняются каждые 100 строк, что позволяет продолжить обработку при сбое
- **Обработка ошибок**: Все ошибки логируются, но не останавливают процесс обработки

### Мониторинг процесса

```bash
# Проверить статус
./check_progress.sh

# Следить за логами в реальном времени
tail -f download_progress.log

# Остановить процесс
kill $(cat download_pid.txt)
```

## Устранение неполадок

### Проблема: Медленная обработка
- Увеличьте `max_workers` для большего количества параллельных потоков
- Уменьшите `delay` между запросами (осторожно, чтобы не перегрузить серверы)

### Проблема: Много неудачных загрузок
- Проверьте доступность URL в датасете
- Увеличьте `timeout` для медленных серверов
- Проверьте логи на наличие специфических ошибок

### Проблема: Процесс прервался
- Промежуточные результаты сохраняются в `*.csv.temp`
- Можно восстановить данные из временного файла
- Перезапустите скрипт - он продолжит с места остановки (если реализовано) или начнет заново

